<!--landing page for bio-->

<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="UTF-8">
    <title>Home</title>
    <link rel="stylesheet" type="text/css" href="styles.css">
</head>

<header>
    <h1>Nick Wornson</h1>
</header>

<main class="container">
    <section id="main-bio">
      <section id="figure">
        
        <img id="bio-image" src="self.jpg" alt="Nick Wornson">
      </section>

      <p> A graduate of the Masters in Statistics program at the University of Minnesota in summer 2019, I have a passion for leveraging data-driven insights.
          I have done a lot of work with a research lab in the Dept. of Plant Pathology at the UMN for the last three years, where the lab performs various experiments, 
          then passes the data off to me for ETL and statistical analysis.  
          At this point in time, all the data and associated analyses are proprietary to the U of M so data will not be included here.  
          I have also helped teach an evening course in Data Visualization, where we teach students a variety of tools ranging from Excel and VBA to Python to Web design.  
          In addition to Data Science, I also enjoy soccer, frisbee golf, science-fiction and cooking.  
          This page (always under construction) is meant to contain a few of my professional and personal projects from the last two years.  </p>

      <h2>Plant Pathology</h2>
      
      <p> What can we learn about the interaction between microbes using optical density readings and genetic composition?  Generally speaking, that was the main question my lab sought to explore.  Sorting through a flurry of scientific terms I had never before heard in my life, I was to perform the data work in support of their research, and advise on statistical methodology.   </p>

      <p> The experiment that I primarily worked on consisted of samples from a research field here in Minnesota.  Across the field were plots (blocks) and within each plot were soil amended plants and control plants crossed with two separate species in sub-plots.  Samples were collected from each sub-plot and examined in the lab for nutrient consumption, inhibition, genetic composition, and a few other measures.  Nutreint consumption was of special interest, not only to us, but to other labs as well.  The process involves using a machine to expose a sample to 85 different nutrients plus a control and record the optical density (ODS).  My first challenge was ETL (Extract, Transform, Load).  I had to aggregate all of the raw xlsx output files from each sample, aggregate them, and merge the resulting data set with experiment data (treatment, species, plot, subplot).  After that we had a few different metrics we were interested in including the number of ODS readings that a given sample scored above a threshold, the mean and variance of ODS readings for a given sample, and whats called Percent Niche Overlap (PNO).  PNO is a useful proxy for measuring competition between microbes.  Mathematically, for samples X and Y, we can calculate Y overlapping X over k isolates using this formula:</p>          
      
      <img class='formula' src="assets/equation.png"/>
      <p>Applying this formula to each isolate combination effectively gives us a distance matrix.  
        Presented with data of this structure, and given we have two types and a treatment, we can apply an unsupervised 
        learning method such as NMDS (Non-metric Multi-Dimensional Scaling), which is common in fields such as Ecology where we want to study community interaction.
        NMDS can give us a visual idea of common variation within and between treatment groups.  Below is an example of an visualizing the first two MDS vector output,
        in this case using euclidean distance between
        pairwise ODS readings.</p>
      <img class='formula' src= 'assets/nmds_deid.png' alt="image missing">
      
      <h2>Flu Shot Learning: Predict H1N1 and Seasonal Flu Vaccines</h2>

      <p>This is a data competition put on by DrivenData where competitors are challenged to predict the probability that an individual would receive an h1n1 and/or a seasonal flu vaccine.  The competition can be found <a href="https://www.drivendata.org/competitions/66/flu-shot-learning/" target="_blank">here</a>.  The data, mostly opinions and behaviors such as flu concern and mask behavior as well as demographics, contains 36 total columns and 26707 observations. </p>

      <p> The competition is purely predictive, so as of yet I have not produced any inference or data visualizations.  Challenges here include missing data, selecting the best model and efficiently coding for two response variables.  Currently I place ~215 out of ~1700 with a score of .8497, the top score being about .866.  Scoring is computed using area under the ROC curve and averaged over seasonal and h1n1.  All my work can be found in this <a href="https://github.com/nwornson/DrivenData-Flu-Shot-Predictions/" target ="_blank">repo.</a>  In addition, below are a couple of R markdown files looking at different approaches to prediction. <p>         <a href="assets\elastic_cv.html"  target="_blank">Elastic-Net Logistic Regression</a>  <br>

      <a href="assets\xgboost.html"  target="_blank">Gradient Boosting with Cross-Validation</a>  
      
      
      <h2>Deep Learning Experiment </h2>
      <h3>Feature Representation With Small Sample Sizes in Convolutional Neural Networks</h3>
      
      <p>The thesis for my Masters details the motivation, research, approach, procedure, and results of a two part project completed in the Spring of 2019 at the University of Minnesota.  This project started out as a contract job for a local company called LAB 651 where we explored deep learning solutions to automated defect detection.  About two months into the job it was realized that training data would not be sufficient by June 2019, and this project would need to be modified to become meaningful.  Anticipating that LAB 651 may need to contend with small sample sizes, we performed a small experiment to see if using different models in the last layer of a Convolutional Neural Network (CNN) could help to battle small sample sizes.  The alternate models included a Random Forest Classifier and a Support Vector Classifier.  Results suggest that they do slightly better on a complex network, and noticeably better on a less complex network. </p>

      <p> Results below show the comparison in accuracy over 5 model fits at each sample size, each model was fit on a random sample taken from the whole training dataset, as well as a plot of the standard errors.  The above graphic shows the results from a more complex CNN model, the lower showing a less complex CNN where the difference in models used on the feature representation can be visibly distinguished.</p>      

      
      <div id = 'chart1'></div>

      <div id = 'chart2'></div>
    </section>
        <aside id="contact-info">
          <h2>Contact Info</h2>
          <ul>
            <li><strong>Email:</strong> <a href="mailto:nick.wornson@gmail.com" target="_blank">Nick.Wornson@gmail.com</a></li>
            <li><strong>Github:</strong> <a href="https://github.com/nwornson" target="_blank">nwornson</a></li>
            <li><strong>Linkedin:</strong> <a href="https://linkedin.com/in/nick-wornson-26b8826a" target="_blank">Nick Wornson</a></li>
            <li><strong></strong> <a href="gallery/index.html" target="_blank">Photo Gallery</a></li>
    
          </ul>
        </aside>


  </main>
  <script src="https://d3js.org/d3.v5.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/d3-tip/0.7.1/d3-tip.min.js"></script>
		
  <script type="text/javascript" src="static/D3_1.js"></script>
  <script type="text/javascript" src="static/D3_2.js"></script>


</body>

</html>