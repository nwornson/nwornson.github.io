<!--landing page for bio-->

<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="UTF-8">
    <title>Home</title>
    <link rel="stylesheet" type="text/css" href="styles.css">
</head>

<header>
    <h1>Nick Wornson</h1>
</header>

<main class="container">
    <section id="main-bio">
      <section id="figure">
        
        <img id="bio-image" src="self.jpg" alt="Nick Wornson">
      </section>

      <p> A graduate of the Masters in Statistics program at the University of Minnesota in summer 2019, I have a passion for leveraging data-driven insights.  I have done a lot of work with a research lab in the Dept. of Plant Pathology at the UMN for the last three years, where the lab performs various experiments, then passes the data off to me for ETL and statistical analysis.  At this point in time, all the data and associated analyses are proprietary to the U of M so data will not be included here.  I have also helped teach an evening course in Data Visualization, where we teach students a variety of tools ranging from Excel and VBA to Python to Web design.  In addition to Data Science, I also enjoy soccer, frisbee golf, science-fiction and cooking.  This page is meant to contain a few of my professional and personal projects from the last two years.  </p>

      <h2>Plant Pathology</h2>
      
      <p> The plot shown below is a pretty typical example of what I might have been expected to do on an average day at the Plant Path lab.  This is the visual output of whats known as Non-Metric Multidimenional Scaling (NMDS), a popular procedure performed in ecology when you are interested in differences between samples.  If you are aware of Principal Component Analysis (PCA), I would describe this as a similar procedure but for a distance matrix rather than a feature matrix.   NMDS seeks to find the directions of greatest variance in order to reduce the dimensionality of the data and allow us to visualize any clustering that might be happening.  In other words it's a method that allows us to see if our treatments and sample types vary together, or at random.  </p>
      <img id="plots" src= 'assets/nmds_deid.png' alt="image missing">
      
      <h2>Flu Shot Learning: Predict H1N1 and Seasonal Flu Vaccines</h2>

      <p>This is a data competition put on by DrivenData where competitors are challenged to predict the probability that an individual would receive an h1n1 and/or a seasonal flu vaccine.  The competition can be found <a href="https://www.drivendata.org/competitions/66/flu-shot-learning/" target="_blank">here</a>.  The data, mostly opinions and behaviors such as flu concern and mask behavior as well as demographics, contains 36 total columns and 26707 observations. </p>

      <p> The competition is purely predictive, so as of yet I have not produced any inference or data visualizations.  Challenges here include missing data, selecting the best model and efficiently coding for two response variables.  Currently I place ~300 out of ~1700 with a score of .8356, the top score being about .866.  Scoring is computed using area under the ROC curve and averaged over seasonal and h1n1.  All my work can be found in this <a href=”https://github.com/nwornson/DrivenData-Flu-Shot-Predictions”" target="_blank">repo</a>  below are R markdown files looking at a couple different approaches to prediction: </p>   

      <a href="assets\elastic_cv.html"  target="_blank">Elastic-Net Logistic Regression</a>  <br>

      <a href="assets\xgboost.html"  target="_blank">Gradient Boosting with Cross-Validation</a>  
      
      
      <h2>Deep Learning Experiment </h2>
      <h3>Feature Representation With Small Sample Sizes in Convolutional Neural Networks</h3>
      
      <p>The thesis for my Masters details the motivation, research, approach, procedure, and results of a two part project completed in the Spring of 2019 at the University of Minnesota.  This project started out as a contract job for a local company called LAB 651 where we explored deep learning solutions to automated defect detection.  About two months into the job it was realized that training data would not be sufficient by June 2019, and this project would need to be modified to become meaningful.  Anticipating that LAB 651 may need to contend with small sample sizes, we performed a small experiment to see if using different models in the last layer of a Convolutional Neural Network (CNN) could help to battle small sample sizes.  The alternate models included a Random Forest Classifier and a Support Vector Classifier.  Results suggest that they do slightly better on a complex network, and noticeably better on a less complex network. </p>

      <p> Results below show the comparison in accuracy over 5 model fits at each sample size, each model was fit on a random sample taken from the whole training dataset, as well as a plot of the standard errors.  The above graphic shows the results from a more complex CNN model, the lower showing a less complex CNN where the difference in models used on the feature representation can be visibly distinguished.</p>      

      
      <div id = 'chart1'></div>

      <div id = 'chart2'></div>
    </section>
        <aside id="contact-info">
          <h2>Contact Info</h2>
          <ul>
            <li><strong>Email:</strong> <a href="mailto:nick.wornson@gmail.com" target="_blank">Nick.Wornson@gmail.com</a></li>
            <li><strong>Github:</strong> <a href="https://github.com/nwornson" target="_blank">nwornson</a></li>
            <li><strong>Linkedin:</strong> <a href="https://linkedin.com/in/nick-wornson-26b8826a" target="_blank">Nick Wornson</a></li>
            <li><strong></strong> <a href="gallery/index.html" target="_blank">Photo Gallery</a></li>
    
          </ul>
        </aside>


  </main>
  <script src="https://d3js.org/d3.v5.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/d3-tip/0.7.1/d3-tip.min.js"></script>
		
  <script type="text/javascript" src="static/D3_1.js"></script>
  <script type="text/javascript" src="static/D3_2.js"></script>


</body>

</html>